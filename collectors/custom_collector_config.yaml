# 🛡️ Ultra SIEM Custom Collector Configuration
# Enterprise-grade configuration for custom log ingestion

# Log format configuration
format: "json" # Options: json, xml, csv, regex

# Source configuration
source:
  type: "file" # Options: file, directory, http
  path: "/var/log/application.log"

  # For HTTP sources
  # type: "http"
  # url: "http://api.example.com/logs"
  # interval: 60  # seconds

# Field mapping from source to Ultra SIEM schema
field_mapping:
  # Core fields
  source_ip: "client_ip"
  destination_ip: "server_ip"
  source_port: "client_port"
  destination_port: "server_port"
  protocol: "protocol"
  event_type: "event_type"
  severity: "severity"
  message: "message"
  user: "username"
  hostname: "host"
  process: "process_name"
  process_id: "pid"
  event_id: "event_id"
  event_category: "category"

  # Metadata fields
  metadata.timestamp: "timestamp"
  metadata.request_id: "request_id"
  metadata.session_id: "session_id"
  metadata.user_agent: "user_agent"
  metadata.referer: "referer"

# Default values for missing fields
defaults:
  log_source: "custom_application"
  severity: 2
  protocol: "http"
  event_category: "application"

# Custom transformations
custom_transforms:
  # Add timestamp if missing
  add_timestamp:
    function: "x.timestamp = int(time.time()) if not x.timestamp else x.timestamp"

  # Normalize severity levels
  normalize_severity:
    function: "x.severity = {'low': 1, 'medium': 2, 'high': 3, 'critical': 4}.get(x.severity.lower(), 2) if isinstance(x.severity, str) else x.severity"

  # Extract IP addresses from message
  extract_ips:
    function: "import re; ips = re.findall(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b', x.message); x.source_ip = ips[0] if ips else x.source_ip; x.destination_ip = ips[1] if len(ips) > 1 else x.destination_ip"

  # Add geographic information
  add_geo:
    function: "x.metadata['geo_country'] = 'US' if x.source_ip.startswith('192.168.') else 'Unknown'"

# NATS configuration
nats:
  url: "nats://admin:ultra_siem_admin_2024@localhost:4222"
  topic: "ultra_siem.events"
  client_name: "custom-collector-001"
  reconnect_time_wait: 1
  max_reconnect_attempts: 10

# HTTP fallback configuration
http:
  url: "http://localhost:8080/events"
  auth_token: "your-auth-token"
  timeout: 5
  retry_attempts: 3

# Logging configuration
logging:
  level: "INFO" # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "/var/log/ultra-siem-custom-collector.log"

# Error handling
error_handling:
  stop_on_error: false
  max_errors: 100
  error_threshold: 0.1 # 10% error rate

# Performance tuning
performance:
  batch_size: 100
  batch_timeout: 5 # seconds
  max_workers: 10
  buffer_size: 10000

# Security configuration
security:
  tls_enabled: false
  tls_cert_file: "/path/to/cert.pem"
  tls_key_file: "/path/to/key.pem"
  tls_ca_file: "/path/to/ca.pem"

# Monitoring configuration
monitoring:
  metrics_enabled: true
  metrics_port: 8080
  health_check_enabled: true
  health_check_interval: 30 # seconds

# Example configurations for different formats

# JSON format example
json_example:
  format: "json"
  field_mapping:
    source_ip: "client_ip"
    event_type: "type"
    severity: "level"
    message: "msg"
    user: "user"
    timestamp: "ts"

# XML format example
xml_example:
  format: "xml"
  field_mapping:
    source_ip: "client_ip"
    event_type: "event_type"
    severity: "severity"
    message: "message"
    user: "user"

# CSV format example
csv_example:
  format: "csv"
  field_mapping:
    source_ip: "client_ip"
    event_type: "event_type"
    severity: "severity"
    message: "message"
    user: "user"

# Regex format example
regex_example:
  format: "regex"
  regex_pattern: r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(\w+)\] (\S+) (\S+) (\S+) (\S+)'
  field_mapping:
    timestamp: "1"
    event_type: "2"
    source_ip: "3"
    destination_ip: "4"
    protocol: "5"
    message: "6"

# Directory monitoring example
directory_example:
  source:
    type: "directory"
    path: "/var/log/applications/"
  format: "json"
  field_mapping:
    source_ip: "client_ip"
    event_type: "type"
    severity: "level"
    message: "msg"

# HTTP monitoring example
http_example:
  source:
    type: "http"
    url: "https://api.example.com/logs"
    interval: 30
  format: "json"
  field_mapping:
    source_ip: "client_ip"
    event_type: "type"
    severity: "level"
    message: "msg"

# Advanced transformation examples
advanced_transforms:
  custom_transforms:
    # Parse complex JSON messages
    parse_complex_json:
      function: |
        if x.message and x.message.startswith('{'):
          try:
            import json
            data = json.loads(x.message)
            x.source_ip = data.get('client_ip', x.source_ip)
            x.user = data.get('user', x.user)
            x.metadata['request_id'] = data.get('request_id', '')
          except:
            pass

    # Add threat intelligence
    add_threat_intel:
      function: |
        if x.source_ip:
          # Simulate threat intelligence lookup
          if x.source_ip in ['192.168.1.100', '10.0.0.50']:
            x.metadata['threat_score'] = 85
            x.metadata['threat_category'] = 'malware'
            x.severity = max(x.severity, 4)

    # Enrich with user context
    enrich_user_context:
      function: |
        if x.user:
          # Simulate user context lookup
          if x.user in ['admin', 'root']:
            x.metadata['user_role'] = 'administrator'
            x.severity = max(x.severity, 3)
          elif x.user in ['guest', 'anonymous']:
            x.metadata['user_role'] = 'guest'
            x.severity = max(x.severity, 2)

    # Parse web application logs
    parse_web_logs:
      function: |
        if x.event_type == 'web_request':
          import re
          # Extract HTTP method and status
          method_match = re.search(r'(\w+) /', x.message)
          status_match = re.search(r' (\d{3}) ', x.message)
          if method_match:
            x.metadata['http_method'] = method_match.group(1)
          if status_match:
            status = int(status_match.group(1))
            x.metadata['http_status'] = status
            if status >= 400:
              x.severity = max(x.severity, 3)

    # Add business context
    add_business_context:
      function: |
        # Add business hours context
        import datetime
        now = datetime.datetime.now()
        if 9 <= now.hour <= 17:
          x.metadata['business_hours'] = True
        else:
          x.metadata['business_hours'] = False
          x.severity = max(x.severity, 2)  # Higher severity outside business hours

# Compliance and audit configuration
compliance:
  data_classification: "internal"
  retention_policy: "90_days"
  encryption_required: true
  audit_logging: true

  # GDPR compliance
  gdpr:
    pii_detection: true
    data_minimization: true
    consent_tracking: true

  # HIPAA compliance
  hipaa:
    phi_detection: true
    access_logging: true
    encryption_at_rest: true

  # SOX compliance
  sox:
    financial_data_detection: true
    access_control_logging: true
    change_management_logging: true

# Integration examples
integrations:
  # Slack notification for high severity events
  slack:
    enabled: true
    webhook_url: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
    severity_threshold: 4

  # Email notification
  email:
    enabled: true
    smtp_server: "smtp.company.com"
    smtp_port: 587
    username: "alerts@company.com"
    password: "your-password"
    recipients: ["security@company.com", "admin@company.com"]
    severity_threshold: 3

  # JIRA ticket creation
  jira:
    enabled: true
    server: "https://company.atlassian.net"
    username: "your-username"
    api_token: "your-api-token"
    project_key: "SEC"
    issue_type: "Bug"
    severity_threshold: 4

# Performance optimization
optimization:
  # Memory management
  memory:
    max_heap_size: "512m"
    gc_threshold: 0.8
    object_pool_size: 1000

  # Network optimization
  network:
    connection_pool_size: 10
    keep_alive: true
    compression: true
    compression_level: 6

  # Processing optimization
  processing:
    parallel_processing: true
    max_threads: 8
    queue_size: 10000
    batch_processing: true
    batch_size: 100
    batch_timeout: 5
